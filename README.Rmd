---
title: "Statistic and Statistical languages"
author: "Eric Miedema"
output:
  html_document:
    toc: true
    keep_md: yes
---
<style>
  .col2 {
    columns: 2 200px;         /* number of columns and width in pixels*/
    -webkit-columns: 2 200px; /* chrome, safari */
    -moz-columns: 2 200px;    /* firefox */
  }
  .col3 {
    columns: 2 100px;
    -webkit-columns: 2 100px;
    -moz-columns: 2 100px;
  }
  .myClass_border
  {
   border: 3px solid black;
    padding: 2px;
    margin: 2px;
  }
</style>

## Statistic and Statistical languages
### Branches of statistics

**Descriptive Statistics**   

* Organizing, Summarizing Information
    + Data and Variables
    + Population and Samples
    + Observational and Experimental Studies
* Graphical methods
    + Categorical Data
    + Numeric Data
* Numerical methods
    + Center of the data
    + Variability
    + Relative Standing
    + Association and Correlation
    + Regression   

**Inferential Statistics**   

* Probability
    + Probability of Events
    + Probability distributions
* Estimation
* Decision making

### Statistical Tools

Excel - Spreadsheet software that uses a grid of cells arranged in numbered rows and letter-named columns to organize data 

Python object-oriented programing language - Computer science language design to be highly extensible with interfaces to existing applications.  Organizes and manipulates data within objects leveraging similar data packages as R. 


**R - Interpreted programing language that use vectors of data elements to organize data into lists or dataframe**  

##  Analytical Methods with practice using R
### Measures of centrality, variability and relative standing
#### Data types and Variables

**Categorical** (or qualitative) if the individual observations are categorical responses.   
**Numerical** (or quantitative) if the individual observations are numerical responses *where numerical operations generally have meaning*.  

* Discrete if the possible values are isolated points on the number line. 
* Continuous if the set of possible values form an entire interval on the number line.   
    
Variable: A variable is any characteristic whose value may change from one individual to another.  

* A **univariate** data set consists of observations on a single variable made on individuals in a sample or population.
* A **bivariate** data set consists of observations on two variables made on individuals in a sample or population.
* A **multivariate** data set consists of observations on two or more variables made on individuals in a sample or population.

**Population**: The entire collection of individuals or objects about which information is desired is called the population.  
**Sample**: A sample is a subset of the population, selected for study in some prescribed manner.  

#### Centrality

The **sample mean** of a numeric sample $x_1,x_2,\ldots,x_n$, usually denoted by $\bar{x}$, is the sum of the observations divided by the number observations in the sample. $\bar{x} = \frac{x_1+x_2+\cdots +x_n}{n} = \frac{\sum (x)}{n}$.  The **population mean** is denoted by $\mu$, is the average of all x values in the entire population.  The **centroid** is the observation, in the sample, that is closest to the mean value.

Problems with the mean  

1.  The mean is not necessarily like any of the observations
2.  The mean is sensitive to outliers

The **sample median** is obtained by first ordering the n observations from smallest to largest (with any repeated values included, so that every sample observation appears in the ordered list). Then finding the middle value if n is odd or the mean of the middle two values if n is even.  A **trimmed mean** is computed by deleting a selected number (or percentage) of values from each end of an ordered list, and finally computing the mean of the remaining values.  The sample median and trimmed mean are not sensitive to outliers.

#### Variability of Data

* Range: Quick, does not represent all the data $\text{Range} = \text{maximum} - \text{minimum}$
* Standard deviation, s: Typical numeric representation of the deviation from the mean
* Quartiles: A measure of variability less sensitive to outliers than s
    + Lower quartile (Q1) = median of the lower half of the data set
    + Upper Quartile (Q3) = median of the upper half of the data set 
    + Interquartile range (iqr) = upper quartile - lower quartile
    + An observations is an outlier if it is more than 1.5 iqr away from the nearest quartile
    + An outlier is **extreme** if it is more than 3 iqr from the nearest quartile and it is a **mild** outlier otherwise

#### Relative Standing

* Frequency for a particular category is the number of times the category appears in the data set.
    + Relative frequency for a particular category is the fraction or proportion of the time that the category appears in the data set
* Percentile - Show percent of the observation in the data set fall at or below that value
* Empirical Rule - Show percentage of observations within 1,2 and 3 standard deviations for a normal distribution
    + Approximately 68% of the observations are within 1 standard deviation of the mean.
    + Approximately 95% of the observations are within 2 standard deviation of the mean.
    + Approximately 99.7% of the observations are within 3 standard deviation of the mean.
* Z Score - Standardized score of number of standard deviations from the mean and used to calculate percentile for a normal distribution.  $\text{Z} = \frac{x- \bar{x}}{S}$

<div class="myClass_border">
<center><h4>Descriptive Statistics</h4></center>  

Numeric Method  | Numeric | Categorical
------------- | ------------- | -------------
Center  | Sample Mean $\bar{x} = \frac{x_1+x_2+\cdots +x_n}{n} = \frac{\sum (x)}{n}$  | Sample proportion of success $\hat{p} = \frac {\text{number of S's in the sample}}{n}$
Variability  | Sample Standard deviation $s = \sqrt{\frac{1}{N-1} \sum_{i=1}^N (x_i - \overline{x})^2}$, Estimating $\sigma = \frac{\sigma_\bar{x}}{\sqrt{n}}$  | Estimating $\mu_\hat{p} = p$, $\sigma_\hat{p} = \sqrt{\frac{p(1-p)}{n}}$ 
Relative Standing Statistic  | $\text{z} = \frac{x- \bar{x}}{\sigma}$, Percentage use `dnorm`  |  

Distributions:   

*  Unimodal,bimodal & multimodal indicate number of clear peaks
*  Symmetric $\text{Mean} = \text{median}$, Positively skewed $\text{Mean} > \text{median}$, Negatively skewed $\text{Mean} < \text{median}$ 

</div>  

### Metrics, targets and benchmarking  

What is a [metric](https://en.wikipedia.org/wiki/Metric)?  A ratio that can be used to evaluate performance over time and between individuals, teams, departments and organizations.

## Analytic Interpretation with practice using R
### Probability

Methods for Determining Probability

1. **The classical approach**: Appropriate for experiments that can be described with equally likely outcomes.
2. **The subjective approach**: Probabilities represent an individual's judgment based on facts combined with personal evaluation of other information.
3. **The relative frequency approach**: An estimate is based on an accumulation of experimental results. This estimate, usually derived empirically, presumes a replicable chance experiment.

**Law of Large Numbers**: As the number of repetitions of a chance experiment increases, the chance that the relative frequency of occurrence for an event will differ from the true probability of the event by more than any very small number approaches zero.   

Two events E and F are said to be **independent** if P(E|F)=P(E).  If E and F are not independent, they are said to be **dependent events**.

Two events are **mutually exclusive** if they have no out-comes in common. The term **disjoint** is also sometimes used to describe events that have no outcomes in common.   

<div class="myClass_border">
<center><h4>Probability Events</h4></center> 
Event|Probability
------------- | ------------
A|$P(A)\in[0,1]$
Not A|$P(A^c)=1-P(A)$, $P(A^c) + P(A) = 1$  
A or B|$P(A\cup B) = P(A)+P(B)-P(A\cap B)$  
A or B|$P(A\cup B) = P(A)+P(B)$ If mutually exclusive
A and B|$P(A\cap B)$ = P(A|B)P(B)$
A and B|$P(A\cap B) = P(A)P(B)$ If independent
A given B|$P(A \mid B) = \frac{P(A \cap B)}{P(B)} = \frac{P(B|A)P(A)}{P(B)}$

If $B_1,B_2,...B_k$ are mutually exclusive events with $P(B_1)+P(B_2) = 1$ then for any event E.   

* Law of Total Probability $P(E)= P(E \cap B_1) + P(E \cap B_2)$  
* Bays Rule $P(B_1|E) = \frac{P(E|B_1)P(B_1)}{P(E|B_1)P(B_1) + P(E|B_2)P(B_2)}$  
* Discrete Probability: $\mu_x = \sum xp(x)$,  Variance $\sigma_x^2 = \sum (x- \mu_x)^2p(x)$, $\sum p(x) = 1$
* Binomial Distribution:  $\mu_x = np  \text{ and } \sigma_x = \sqrt{np(1-p)}$, $P(x) = \binom n x  p^x(1-p)^{n-x} \text{ where } \binom n x =\frac{n!}{x!(n-x)!}$
* Geometric Distibution: $P(x) = p(1-p)^{(x-1)}$
* Continuous Probability distribution: Normal, Uniform, etc.  Total area under the density curve is equal to 1.
</div>

### Sampling Distrubtion
Any quantity computed from values in a sample is called a **statistic**.

The observed value of a statistic depends on the particular sample selected from the population; typically, it varies from sample to sample. This variability is called **sampling variability**.  The distribution of a statistic is called its **sampling distribution**.

When random samples are selected form a population, the following are properties of the sampling distribution of $\bar{x}$.  

**Central Limit Theorem**: If n is large (n exceeds 30) or the population distribution is normal, the standardized variable Z has (approximately) a standard normal (z) distribution regardless of the underlying distribution.

A **point estimate** of a population characteristic is a single number that is based on sample data and represents a plausible value of the characteristic

A  **confidence interval** an interval of plausible values for a population characteristic. It is constructed so that, with a chosen degree of confidence, the value of the characteristic will be captured inside the interval.
* The **bound on error estimation**, B, represents the confidence interval quantity.  Also called the **margin of error**.
* The sample size, n, can be calulcated with the desired confidence interval and bound of error.

The **confidence level** is the success rate of the method used to construct a confidence interval.

When $\sigma$ is unknown $\mu$ can be estimated with the Student's **t distribution** depending on the **degrees of freedom** which is the number of values in the final calculation of a statistic that are free to vary.

### Hypothesis Testing

The **null hypothesis**, denoted $H_0$ is a statement or claim about a population characteristic that is initially assumed to be true.   

The **alternate hypothesis**, denoted by $H_a$ is the competing claim.   
The alternate hypothesis is a statement about the same population characteristic that is used in the null hypothesis   

Typically one assumes the null hypothesis to be true and then one of the following conclusions are drawn.  

1.  Reject $H_0$
    + Equivalent to saying that $H_a$ is correct or true
2.  Fail to reject $H_0$
    + Equivalent to saying that we have failed to show a statistically significant deviation from the claim

Hypothesis form:     

$H_0$: population characteristic = hypothesized value   
where the hypothesized value is a specific number determined by the problem context.  

The alternative (or alternate) hypothesis will have one of the following three forms:   

* $H_a$: population characteristic > hypothesized value
* $H_a$: population characteristic < hypothesized value
* $H_a$: population characteristic $\ne$ hypothesized value

Errors in Hypothesis Testing:   

**Type I error (False Positive)** : The error of rejecting $H_0$ when $H_0$ is true.  Probability of a Type I error is denoted $\alpha$ and is called the **level of significance** of the test.  

**Type II error (False Negative)**: The error of failing to rejected $H_0$ when $H_0$ is false.  Probability of a Type II error is denoted by $\beta$.  

Refer to wiki on [Type I & II errors](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors) in healthcare the results for labs or risk algortims are evaluated by the positive or negative desscribed as the [sensitivity and specificity](https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values).  Addiitonally, algorithms are evaluated with the Accuracy (ACC) and the [ROC curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic).

A **test statistic** is the function of sample data on which a conclusion to reject or fail to reject $H_0$ is based.   

The  **P-value** (also called the **observed significance level**) is a measure of inconsistency between the hypothesized value for a population characteristic and the observed sample. A decision as to whether $H_0$ should be rejected results from comparing the P-value to the chosen $\alpha$:

* Reject $H_0$  if P-value $\le \alpha$.
* Fail to reject $H_0$ if P-value > $\alpha$.   

The **power of a test** is the probability of rejecting the null hypothesis

When $H_0$ is false, the power is the probability that the null hypothesis is rejected. Specifically,  power = 1 - $\beta$.

Steps in a Hypothesis-Testing Analysis: 

1.  Describe (determine) the population characteristic about which hypotheses are to be tested.
2.  State the null hypothesis $H_0$.
3.  State the alternate hypothesis $H_a$.
4.  Select the significance level $\alpha$ for the test.
5.  Display the test statistic to be used, with substitution of the hypothesized value identified in step 2 but without any computation at this point.
6.  Check to make sure that any assumptions required for the test are reasonable.
7.  Compute all quantities appearing in the test statistic and then the value of the test statistic itself.
8.  Determine the P-value associated with the observed value of the test statistic
9.  State the conclusion in the context of the problem, including the level of significance.

<div class="myClass_border">
<center><h4>Sample Distribution and Hypothesis Testing</h4></center>  

1. Distribution of mean $\bar{x}$: $\mu_\bar{x} = \mu$, $\sigma_\bar{x} = \frac{\sigma}{\sqrt{n}}$
2. Distribution of proportion $\hat{p}$: $\mu_\hat{p} = p$, $\sigma_\hat{p} = \sqrt{\frac{p(1-p)}{n}}$

Estimate | Bound on Error (B) | Confidence Interval | n with 95% confidence | Notes
------------- | ------------ | ------------ | ------------ | ------------
$\text{z} = \frac{x- \bar{x}}{\sigma}$ | $(\text{z critical value})*(\frac{\sigma}{\sqrt{n}})$ | $\bar{x} \pm B$ | $n = (\frac{1.96\sigma}{B})^2$ | Critical Value of 95%=1.96
$\text{z} = \frac{\hat{p}- p}{\sqrt{\frac{p(1-p)}{n}}}$ | $(\text{z critical value})*\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}$ | $\hat{p} \pm B$ | $n = p(1-p)(\frac{1.96}{B})^2$ | Use p=.5 for conservative estimates of n and always round up.
$t = \frac{\bar{x}-\mu}{\frac{s}{\sqrt{n}}}$ | $(\text{t critical value})*(\frac{s}{\sqrt{n}})$ | $\bar{x} \pm B$ | | df = n-1 and see table for t critical values
</div>

### Association and Correlation

**Positive Association** - Two variables are positively associated when above-average values of one tend to accompany above-average values of the other and below-average values tend similarly to occur together. (i.e., Generally speaking, the y values tend to increase as the x values increase.)

**Negative Association** - Two variables are negatively associated when above-average values of one accompany below-average values of the other, and vice versa. (i.e., Generally speaking, the y values tend to decrease as the x values increase.)

Pearson **Correlation Coefficient**, r, is a measure of the strength of the linear relationship between the two variables is called the Pierson correlation coefficient.  

**Association or correlation DOES NOT indicate causation**


### Regression and Prediction

Object of a regression analysis is to use information about one variable, x, predict the value of a second variable, y.  Multiple regression uses multiple variables to predict y.

The most widely used criterion for measuring the goodness of fit of a line **y = a + bx** to bivariate data $(x_1, y_1), (x_2, y_2),(x_n,y_n)$ is the sum of the of the squared deviations about the line.

$$SSD = \sum ^n _{i=1} (y_i - (a - bx_i))^2$$   

The line that gives the best fit to the data is the one that minimizes this sum; it is called the **least squares** line or **simple regression** line. The equation is represented by **$\hat{y} = a + bx$**.   

The **predicted** or **fitted** values result form substituting each sample x value into the equation for the least squares line.  This gives

$$\hat{y}_1 = a + bx_1 = \text{1st predicted value}$$
$$\hat{y}_2 = a + bx_2 = \text{2nd predicted value}$$
$$\hat{y}_n = a + bx_n = \text{nth predicted value}$$

The **residuals** for the least squares line are the values: $y_1-\hat{y}_1, y_2-\hat{y}_2,...,y_2-\hat{y}_2$   

The **coefficient of determination**, denoted by $r^2$, gives the proportion of variation in y that can be attributed to an approximate linear relationship between x and y.  Note that the coefficient of determination is the square of the Pearson correlation coefficient.   

The **standard deviation about the least squares line** is denoted $s_e$ is interpreted as the "typical" amount by which an observation deviates from the least squares line.   

A general additive **multiple regression** model $y = \alpha + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_k x_k + e$   

* Polynomial regression model $y = \alpha + \beta_1 x + \beta_2 x^2 + \dots + \beta_k x^k + e$   
* Quadratic regression model $y = \alpha + \beta_1 x + \beta_2 x^2$   

**Dummy variables** or **indicator variables** can be created to utilize categorical variables in a regression model.   

The **adjusted R-Squared** adjusts for the amount of explanatory variables (k) added to the model relative to data points (n).  

Regression steps for Bivariate Numeric Dataset   

1. Summarize with scatterplot
2. Determine linear relationship
3. Find equation of least-squares regression line
4. Construct residual plot to look for patterns
5. Compare $s_e$ and $r^2$
6. Decide if least-squares line is useful for predictions
7. Use regression line for prediction   

<div class="myClass_border">
<center><big>Correlation and Regression</big></center>  
<div class="col2">
<p>
Correlation Coefficient $r = r_{xy} = \frac{\sum (Z_x Z_y)}{n-1} = \frac{1}{n-1} \sum ^n _{i=1} \left( \frac{x_i - \bar{x}}{s_x} \right) \left( \frac{y_i - \bar{y}}{s_y} \right)$   

Least Squares Line **$\hat{y} = a + bx$** where  

* Slope of $b = \frac{S_{xy}}{S_{xx}} = \frac{\sum ((x -\bar{x})(y -\bar{y}))}{\sum (x -\bar{x})^2}$
* Intercept of $a = \bar{y} - b\bar{x}$   

Residual sum of squares   
$SSResid = \sum ^n _{i=1} (y_i - (a + bx_i))^2 = \sum ^n _{i=1} (y_i - \hat{y})^2$   
Total sum of squares, $SSTo = \sum ^n _{i=1} (y_i - \bar{y})^2$     
</p>
<p>
Standard deviation about the least squares line   
$s_e = \sqrt{\frac{SSResid}{n-2}} \text{ or for multiple regression } s_e = \sqrt{\frac{SSResid}{n-(k+1)}}$   
Standard deviation about the slope (b), confidence interval df = n - 2   
$s_b = \frac{S_e}{\sqrt{S_{xx}}}, b \pm (\text{t critical value}) s_b$   
Coefficient of determination 

*  $r^2 = 1 - \frac{\text{SSResid}}{\text{SSTo}}$   
*  Adjusted $r^2 = 1 - [\frac{n-1}{n-(k+1)}] \frac{\text{SSResid}}{\text{SSTo}}$
</p>
</div>
</div>