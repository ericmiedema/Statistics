---
title: "Bivariate Data"
author: "Eric Miedema"
output: ioslides_presentation
widescreen: true
---

## Statistic Class Week 4

![](Pictures/dilbert_causation.png)

### Bivariate Data: Understanding relationship between variables
* Scatter plots
* Correlation
* Regression - Line of best fit
    
# Chapter 5 - Bivariate Data

## Terms Review {.smaller}

A **bivariate** data set consists of observations on two variables made on individuals in a sample or population. A *scatterplot* is a plot of pairs of observed values (both quantitative) of two different variables  

* A **response variable** is thought to be related to the explanatory variable in the experiment, but not controlled by the experimenter.  In a bivariate data set this is called the **dependent variable** and is the value we would like to predict.  The dependent variable is denoted by y.  

* An **explanatory variable** are those variables that have values that are controlled by the experimenter.  Also called factors.  In a bivariate data set this is called the **independent variable** or **predictor variable** and is used to make a prediction of the dependent variable.  The independent variable is denoted by x.  

**Positive Association** - Two variables are positively associated when above-average values of one tend to accompany above-average values of the other and below-average values tend similarly to occur together. (i.e., Generally speaking, the y values tend to increase as the x values increase.)

**Negative Association** - Two variables are negatively associated when above-average values of one accompany below-average values of the other, and vice versa. (i.e., Generally speaking, the y values tend to decrease as the x values increase.)

## Correlation Coeficient

Pearson **Correlation Coefficient** is a measure of the strength of the linear relationship between the two variables is called the Pierson correlation coefficient.  

The Pearson sample correlation coefficient is defined by

$$r = r_{xy} = \frac{\sum (Z_x Z_y)}{n-1} = \frac{1}{n-1} \sum ^n _{i=1} \left( \frac{x_i - \bar{x}}{s_x} \right) \left( \frac{y_i - \bar{y}}{s_y} \right)$$

## Correlation Coeficient Example

```{r echo = FALSE, warning=FALSE, message=FALSE, results='hide'}
library(googleVis)
op <- options(gvis.plot.tag='chart')
x<-round(rnorm(10,mean = 150, sd = 10),0)
y<-round(x+rnorm(10,mean = 0, sd = 5),0)
df<-data.frame(x = x, y = y)
#df$Z_x<-(df$x-mean(df$x))/sd(x)
#df$Z_y<-(df$y-mean(df$y))/sd(y)
#df$product_z<-df$Z_x*df$Z_y
```

```{r echo = FALSE, warning=FALSE, message=FALSE, results='asis'}
Table <- gvisTable(df)
plot(Table)
```

```{r echo = FALSE, warning=FALSE, message=FALSE, results='hide'}
df$Z_x<-(df$x-mean(df$x))/sd(x)
df$Z_y<-(df$y-mean(df$y))/sd(y)
df$product_z<-df$Z_x*df$Z_y
```

<div class="notes">
* n = 10
* $\bar{x}$  = `r round(mean(x),1)` and $s_x$ = `r round(sd(x),1)`
* $\bar{y}$ = `r round(mean(y),1)` and $s_x$ = `r round(sd(y),1)`
* The sum is `r round(sum(df$product_z),1)` divided by 9 to get r = `r round(sum(df$product_z)/9,2)`
</div>
<br>
  
The sum of the product of Z scores is `r round(sum(df$product_z),1)` divided by 9 to get r = `r round(sum(df$product_z)/9,2)`.  Or use =corrrel() function in statistical software.

```{r warning=FALSE, message=FALSE}
cor(x,y)
```

## Correlation 

```{r echo = FALSE, warning=FALSE, message=FALSE}
library(ggplot2)
library(gridExtra)
x<-rnorm(30,mean = 5, sd = 1)

y<-10-(x-rnorm(30,mean = 0, sd = 5))
df<-data.frame(x = x, y = y)
plot1<-ggplot(df, aes(x,y)) + geom_point() + labs(title = paste("r of",round(cor(x,y),2),"with Neg Assoc.",sep = " "))
y<-10-(x-rnorm(30,mean = 0, sd = 2))
df<-data.frame(x = x, y = y)
plot2<-ggplot(df, aes(x,y)) + geom_point()  + labs(title = paste("r of",round(cor(x,y),2),"with Neg Assoc.",sep = " "))
y<-x+rnorm(30,mean = 0, sd = .5)
df<-data.frame(x = x, y = y)
plot3<-ggplot(df, aes(x,y)) + geom_point() + labs(title = paste("r of",round(cor(x,y),2),"with Pos Assoc.",sep = " "))
y<-x
df<-data.frame(x = x, y = y)
plot4<-ggplot(df, aes(x,y)) + geom_point() + labs(title = paste("r of",round(cor(x,y),2),"with Pos Assoc.",sep = " "))
grid.arrange(plot1, plot2, plot3, plot4, ncol=2, nrow = 2)
```

## Properties of r

* The value of r does not depend on the unit of measurement for each variable.
* The value of r does not depend on which of the two variables is labeled x.
* The value of r is between -1 and +1.
* The correlation coefficient is 
    + -1 only when all the points lie on a downward-sloping line, and 
    + +1 only when all the points lie on an upward-sloping line.
* The value of r is a measure of the extent to which x and y are linearly related.

## Correlations

Correlation coefficient  

* Advantage - Provides a quantitative measure of the degree of relatedness between two variables
* Disadvantage - Not very effective for non linear relationships

Scatter plot  

* Advantages - Instantly shows the relationship between two variables
* Disadvantages - The relationship is described by a bunch of data points

Next step is a line to represent the data

## Linear regression

Object of a regression analysis is to use information about one variable, x, predict the value of a second variable, y.
Multiple regression uses multiple variables to predict y.

The relationship **y = a + bx** is the equation of a straight line. 

*  The value **b**, called the slope of the line, is the amount by which y increases when x increase by 1 unit. 
*  The value of **a**, called the intercept (or sometimes the vertical intercept) of the line, is the height of the line above the value x = 0.

## Least Squares Line

The most widely used criterion for measuring the goodness of fit of a line   
**y = a + bx** to bivariate data $(x_1, y_1), (x_2, y_2),(x_n,y_n)$ is the sum of the of the squared deviations about the line.

$$SSD = \sum ^n _{i=1} (y_i - (a - bx_i))^2$$

The line that gives the best fit to the data is the one that minimizes this sum; it is called the **least squares** line or **sample regression** line.  

## Calculating Least Squares Line

The equation is represented by **$\hat{y} = a + bx$** where

* Slope of $b = \frac{1}{\sum (x -\bar{x})^2} \sum ((x -\bar{x})(y -\bar{y}))$
* Intercept of $a = \bar{y} - b\bar{x}$

## Simple Regression Line Example

```{r echo = FALSE, warning=FALSE, message=FALSE, results='hide'}
library(googleVis)
op <- options(gvis.plot.tag='chart')
x<-round(rnorm(10,mean = 150, sd = 10),0)
y<-round(2*x+rnorm(10,mean = 0, sd = 5),0)
df<-data.frame(x = x, y = y)
```

```{r echo = FALSE, warning=FALSE, message=FALSE, results='asis'}
Table <- gvisTable(df)
plot(Table)
```

```{r echo = FALSE, warning=FALSE, message=FALSE, results='hide'}
df$DEV_X<-(df$x-mean(df$x))
df$DEV_Y<-(df$y-mean(df$y))
df$PROD_DEV<-df$DEV_X*df$DEV_Y
df$SQR_DEV_X<-df$DEV_X^2
b<-round(sum(df$PROD_DEV)/sum(df$SQR_DEV_X),3)
a<-round(mean(y)- b*mean(x),3)
df$Predicted<-a+b*df$x
df$Residual<-df$y-df$Predicted
```

<div class="notes">
* n = 10
* $\bar{x}$  = `r round(mean(x),1)` and $s_x$ = `r round(sd(x),1)`
* $\bar{y}$ = `r round(mean(y),1)` and $s_x$ = `r round(sd(y),1)`
* r = `r cor(x,y)`
* b = `r sum(df$PROD_DEV)` / `r sum(df$SQR_DEV_X)` = `r b`
* a = `r round(mean(y),1)` - `r round(mean(x),1)` * `r b` = `r a`
</div>
  
$$\hat{y} = `r a` + `r b` * x$$

## Example plot with regression
```{r echo = FALSE, warning=FALSE, message=FALSE, results='hide', fig.height= 5, fig.width=9}
library(ggplot2)
library(gridExtra)

p <- ggplot(df, aes(x=x, y=y))
plot1<-p + geom_point() + geom_abline(intercept = a, slope = b) + labs(title = paste("Intercept = ",a," Slope = ",b,sep = ""))

lm_eqn <- function(df, x = x){
    m <<- lm(y ~ x, df)
    a <<- format(coef(m)[1], digits = 2)
    b <<- format(coef(m)[2], digits = 2)
    r2 <<- format(summary(m)$r.squared, digits = 3)
}
lm_eqn(df)

plot2<-p + geom_point() + geom_smooth(method = "lm") + labs(title = paste("r2 = ",r2,", y = ",a," + ",b,"x", sep = ""))
plot3<-ggplot(df, aes(x=x, y=Residual)) + geom_point() + labs(title = paste("Residual Plot", sep = ""))
grid.arrange(plot1, plot2,plot3, ncol=3)
```

## Assessing the fit of a line

The **predicted** or **fitted** values result form substituting each sample x value into the equation for the least squares line.  This gives

$$\hat{y}_1 = a + bx_1 = \text{1st predicted value}$$
$$\hat{y}_2 = a + bx_2 = \text{2nd predicted value}$$
$$\hat{y}_n = a + bx_n = \text{nth predicted value}$$

The **residuals** for the least squares line are the values: $y_1-\hat{y}_1, y_2-\hat{y}_2,...,y_2-\hat{y}_2$

## Residual Plot

A **residual plot** is a scatter plot of the data pairs (x, residual).

```{r echo = FALSE, warning=FALSE, message=FALSE, fig.width = 5, fig.height = 2.5}
p <- ggplot(df, aes(x=x, y=Residual))
p + geom_point()
```

* Isolated points or patterns indicate potential problems
* Ideally the the points should be randomly spread out above and below zero.
* Patterns, such as curves or waves, indicate a nonlinear regression model
* Values below 0 indicate over predication

## Coefficient of determination

The **coefficient of determination**, denoted by $r^2$, gives the proportion of variation in y that can be attributed to an approximate linear relationship between x and y.  Additionally the adjusted $r^2$ can describe the relationship for multiple variables and y by adjusting for the number for predictors in the regression. 

Note that the coefficient of determination is the square of the Pearson correlation coefficient.

## Calculating $r^2$

The coefficient of determination, $r^2$, can be computed as 
$$r^2 = 1 - \frac{\text{SSResid}}{\text{SSTo}}$$

Where the **total sum of squares**, denoted **SSTo** is
$$SSTo = \sum ^n _{i=1} (y_i - \bar{y})^2$$
Where the residual sum of squares, denoted **SSResid** is
$$SSResid = \sum ^n _{i=1} (y_i - (a - bx_i))^2 = \sum ^n _{i=1} (y_i - \hat{y})^2$$

## Ploting with R Squared
```{r echo = FALSE, warning=FALSE, message=FALSE}
x<-rnorm(30,mean = 5, sd = 1)

y<-90-(x-rnorm(30,mean = 0, sd = 5))
df<-data.frame(x = x, y = y)
lm_eqn(df)
plot1<-ggplot(df, aes(x,y)) + geom_point() + geom_smooth(method = "lm") + labs(title = paste("r2 = ",r2,", y = ",a," + ",b,"x", sep = ""))
y<-90-(x-rnorm(30,mean = 0, sd = 2))
df<-data.frame(x = x, y = y)
lm_eqn(df)
plot2<-ggplot(df, aes(x,y)) + geom_point() + geom_smooth(method = "lm") + labs(title = paste("r2 = ",r2,", y = ",a," + ",b,"x", sep = ""))
y<-x+rnorm(30,mean = 0, sd = .5)
df<-data.frame(x = x, y = y)
lm_eqn(df)
plot3<-ggplot(df, aes(x,y)) + geom_point() + geom_smooth(method = "lm") + labs(title = paste("r2 = ",r2,", y = ",a," + ",b,"x", sep = ""))
y<-x
df<-data.frame(x = x, y = y)
lm_eqn(df)
plot4<-ggplot(df, aes(x,y)) + geom_point() + geom_smooth(method = "lm") + labs(title = paste("r2 = ",r2,", y = ",a," + ",b,"x", sep = ""))
grid.arrange(plot1, plot2, plot3, plot4, ncol=2, nrow = 2)
```

<div class="notes">
For the r2 of 1, we can say that 100% of the variation in the $\hat{y}$ can be attributed to the least squares linear relationship between x and y.
</div>

## Standard deviation about the least squares line
The **standard deviation about the least squares line** is denoted $s_e$ and given by 

$$s_e = \sqrt{\frac{SSResid}{n-2}}$$

$s_e$  is interpreted as the "typical" amount by which an observation deviates from the least squares line. 

<div class="notes">
Whey n-2?  Degree of freedom is now 2 because of 2 variables (slope and intercept) used in the regression line instead of comparing to just the mean of one variable.
</div>

## Nonlinear Relationships and Transformations

```{r echo = FALSE, warning=FALSE, message=FALSE}
library(ggplot2)
library(gridExtra)
x<-runif(100,min = 0, max = 100)
rand<-x+rnorm(100,mean = 3,sd = .5)

y<-x^2+3*rand+10
df<-data.frame(x = x, y = y)
plot1<-ggplot(df, aes(x,y)) + geom_point() + stat_smooth(method="lm", se=TRUE, fill=NA,
                formula=y ~ poly(x, 2, raw=TRUE),colour="red") + labs(title = paste("Quadratic",sep = " "))
y<-1/rand+10
df<-data.frame(x = x, y = y)
lm_eqn(df,x = 1/x)
plot2<-ggplot(df, aes(x,y)) + geom_point() + stat_smooth(method="lm", se=TRUE, fill=NA,
                formula=y ~ (1/x),colour="red") + labs(title = paste("Reciprocal of x",sep = " "))
y<-2*sqrt(rand)+10
df<-data.frame(x = x, y = y)
plot3<-ggplot(df, aes(x,y)) + geom_point() + stat_smooth(method="lm", se=TRUE, fill=NA,
                formula=y ~ sqrt(x),colour="red") + labs(title = paste("Square Root of x",sep = " "))
y<-2*log(rand)+10
df<-data.frame(x = x, y = y)
plot4<-ggplot(df, aes(x,y)) + geom_point() + stat_smooth(method="lm", se=TRUE, fill=NA,
                formula=y ~ log(x),colour="red") + labs(title = paste("Log of x",sep = " "))
grid.arrange(plot1, plot2, plot3, plot4, ncol=2, nrow = 2)
```

## Regression for Bivariate Numeric Dataset

1. Summarize with scatterplot
2. Determine linear relationship
3. Find equation of least-squares regression line
4. Construct residual plot to look for patterns
5. Compare $s_e$ and $r^2$
6. Decide if least-squares line is useful for predictions
7. Use regression line for prediction

## Summary Formulas for Bivariate Data {.smaller}

Correlation Coefficient $r = r_{xy} = \frac{\sum (Z_x Z_y)}{n-1} = \frac{1}{n-1} \sum ^n _{i=1} \left( \frac{x_i - \bar{x}}{s_x} \right) \left( \frac{y_i - \bar{y}}{s_y} \right)$   

Least Squares Line **$\hat{y} = a + bx$** where  

* Slope of $b = \frac{S_{xy}}{S_{xx}} = \frac{\sum ((x -\bar{x})(y -\bar{y}))}{\sum (x -\bar{x})^2}$
* Intercept of $a = \bar{y} - b\bar{x}$   

Evaluating the Least Squares Line  

* Residual sum of squares, $SSResid = \sum ^n _{i=1} (y_i - (a + bx_i))^2 = \sum ^n _{i=1} (y_i - \hat{y})^2$   
* Total sum of squares, $SSTo = \sum ^n _{i=1} (y_i - \bar{y})^2$  
* Standard deviation about the least squares line, $s_e = \sqrt{\frac{SSResid}{n-2}}$  
* Coefficient of determination, $r^2 = 1 - \frac{\text{SSResid}}{\text{SSTo}}$   

## Homework Chapter 5

* 5.1, 5.2, 5.9, 
* 5.15, 5.16, 5.26
* 5.29, 5.33, 5.47

Excel Challenge: Using the [Cars Data](https://docs.google.com/spreadsheets/d/1HGtm0XKSBjoGBfcyC7D7O7y7iqfTPcR0hY9v7uh9mS8/edit?usp=sharing) , fit a regression for car weight on mpg.  Include:

1.  Scatterplot with line of best fit
2.  Calculations for correlation (r), slope (b), intercept (a), Coefficient of determination ($r^2$) and standard deviation about the least squares line ($s_e$)
3.  Residual plot
4.  Predicted MPG for 2.5 ton car

